#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 15 10:24:02 2025

@author: gamal
"""

def train_som(X, W, G, n_iterations=10000, sigma=1.5, eta=0.5):
    for t in range(n_iterations):
        x = X[np.random.randint(0, X.shape[0])]
        
        #BMU
        diffs = W - x  
        dist = np.linalg.norm(diffs, axis=2)
        bmu = np.unravel_index(np.argmin(dist), dist.shape)
        for n in G.nodes():
            dist = nx.shortest_path_length(G, source=bmu,target=n)
            h = np.exp(-(dist / (sigma ** 2)))
            i, j = n
            W[i, j] += eta * h * (x - W[i, j])
    return W

def train_som_optimized(X, W, distance_matrix, n_iterations=100, sigma=1.5, eta=0.5, batch_size=10):
    m, k, dim = W.shape
    for t in range(n_iterations):
        batch = X[np.random.choice(X.shape[0], size=batch_size, replace=False)]
        delta_W = np.zeros_like(W)
        for x in batch:
            dists = np.linalg.norm(W - x, axis=2)
            bmu = np.unravel_index(np.argmin(dists), dists.shape)
            distances = distance_matrix[bmu[0], bmu[1]]
            h = np.exp(-distances / (sigma ** 2))[:, :, np.newaxis]
            delta_W += eta * h * (x - W)
        W += delta_W / batch_size
    return W

def train_som_epochwise(X, W, distance_matrix, n_epochs=100, sigma=1.5, eta=0.5, batch_size=64):
    m, k, dim = W.shape
    n_samples = X.shape[0]
    
    prev_bmus = np.zeros((n_samples, 2), dtype=int)
    bmu_movement = []
    bmu_switch_count = []
    
    for epoch in range(n_epochs):
        X_shuffled = X[np.random.permutation(X.shape[0])]
        current_bmus = np.zeros((n_samples, 2), dtype=int)
        for i in range(0, n_samples, batch_size):
            batch = X_shuffled[i:i+batch_size]
            delta_W = np.zeros_like(W)
            for x in batch:
                dists = np.linalg.norm(W - x, axis=2)
                bmu = np.unravel_index(np.argmin(dists), dists.shape)
                distances = distance_matrix[bmu[0], bmu[1]]
                h = np.exp(-distances / (sigma ** 2))[:, :, np.newaxis]
                delta_W += eta * h * (x - W)
            W += delta_W / batch.shape[0]
    return W


def train_som_with_convergence(X, W, distance_matrix, n_epochs=100, sigma=1, eta=0.5, min_batch_size=1):
    m, k, dim = W.shape
    n_samples = X.shape[0]
    initial_eta, initial_sigma = eta, sigma
    batch_size = X.shape[0]//2
    prev_bmus = np.zeros((n_samples, 2), dtype=int)
    stats = {
        "bmu_movement": [],
        "bmu_switches": [],
        "quantization_error": [],
        "topographic_error": []
    }

    for epoch in range(n_epochs):
        bmu_influence = min(0, 1 / (1 + np.exp(-10 * (epoch/n_epochs - 0.5))))
        shuffled_indices = np.random.permutation(n_samples)
        X_shuffled = X[shuffled_indices]
        current_bmus = np.zeros((n_samples, 2), dtype=int)
        
        for i in range(0, n_samples, batch_size):
            batch_slice = slice(i, min(i + batch_size, n_samples))
            batch = X_shuffled[batch_slice]
            original_indices = shuffled_indices[batch_slice]
            W = update_weights(batch, original_indices, W, distance_matrix, current_bmus, eta, sigma, bmu_influence)

        movement, switch_count, prev_bmus = compute_bmu_metrics(prev_bmus, current_bmus)
        stats["bmu_movement"].append(movement)
        stats["bmu_switches"].append(switch_count)
        stats["quantization_error"].append(quantization_error(W, X))
        stats["topographic_error"].append(topographic_error(W, X))
        
        switch_rate = switch_count / n_samples
        eta, sigma = update_learning_rates(epoch, n_epochs, initial_eta, initial_sigma)
        if switch_rate > 0.5:
            eta *= 1.00
            sigma *= 0.98
        elif switch_rate > 0.2:
            eta *= 0.95
            sigma *= 0.95
        else:
            eta *= 0.90
            sigma *= 0.90

        if switch_rate < 0.3:
            batch_size = max(int(batch_size * 0.9), min_batch_size)
        else:
            batch_size = min(int(batch_size * 1.05), X.shape[0]//2)

        plot_rgb_som(W)
        
    return W, stats


# # UMAP
# reducer = umap.UMAP()
# embedding = reducer.fit_transform(X_skewed)
# n_clusters = 9
# kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(embedding)
# m, k = 3, 3
# n_clusters = m * k
# kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(embedding)
# labels = kmeans.labels_

# # Compute RGB mean for each cluster
# W_init = np.zeros((m, k, 3))
# for i in range(n_clusters):
#     cluster_rgb_values = X_skewed[labels == i]
#     if len(cluster_rgb_values) > 0:
#         mean_rgb = cluster_rgb_values.mean(axis=0)
#     else:
#         mean_rgb = np.random.rand(3)  # Fallback if a cluster ends up empty
#     W_init[i // k, i % k] = mean_rgb
# plot_rgb_som(W_init)



# # --- GPM STORM Dataset ---

# filepath = ("/ltenas2/data/GPM_STORM_DB/merged/merged_data_total_0.parquet") 
# df = pd.read_parquet(filepath) 
# som_dir = os.path.expanduser("~/gpm_storm/scripts")  
# vars = df.columns[0:-9]
# df_scaled = preprocess_data(df, vars)
# df_sample = df_scaled.sample(n=100000, random_state=42)
# X = df_sample.to_numpy()

# m, k = 10, 10
# dim = X.shape[1]
# W = np.random.rand(m, k, dim)
# distance_matrix = precompute_distances(m, k)
# G = nx.grid_2d_graph(m, k)

# st = time.time()
# W_trained, metrics = train_som_with_convergence(X, W, distance_matrix, n_epochs=100)
# et = time.time()
# tt = et-st
# print(tt)

# plt.plot(metrics["bmu_movement"], label="BMU Movement Distance")
# plt.plot(metrics["bmu_switches"], label="BMU Switch Count")
# plt.xlabel("Epoch")
# plt.ylabel("Metric Value")
# plt.title("SOM Convergence Metrics")
# plt.legend()
# plt.grid(True)
# plt.show()




# # --- VISUAL ----
# VARIABLE = "precipRateNearSurface"
# HEATMAP_VARIABLE = "P_mean"
# SOM_SHAPE = (10, 10)
# NUM_IMAGES = 25
# zarr_directory = "/ltenas2/data/GPM_STORM_DB/zarr" 

# def get_patch_dataset(granule_id, patch_id, time, cache):
#     year, month = time.year, time.month
#     if granule_id in cache:
#         ds = cache[granule_id]
#     else:
#         search_path = os.path.join(zarr_directory, f"{year:04d}/{month:02d}", "*.zarr")
#         zarr_files = glob.glob(search_path)
#         for zarr_file in zarr_files:
#             if granule_id in os.path.basename(zarr_file):
#                 ds = xr.open_zarr(zarr_file)
#                 cache[granule_id] = ds
#                 break
#         else:
#             return None
#     return ds.isel(patch=patch_id)


# def create_node_image_array(weights, df):
#     n_rows, n_cols = weights.shape[:2]
#     arr_df = np.empty((n_rows, n_cols), dtype=object)
#     arr_ds = np.empty((n_rows, n_cols), dtype=object)

#     for r in range(n_rows):
#         for c in range(n_cols):
#             mask = (df["row"] == r) & (df["col"] == c)
#             arr_df[r, c] = df[mask].reset_index(drop=True)
#             arr_ds[r, c] = None  # We'll populate this later if needed
#     return arr_df, arr_ds



# def plot_som_grid(arr_ds, cmap="turbo", norm=None):
#     n_rows, n_cols = arr_ds.shape
#     fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols, n_rows))

#     for r in range(n_rows):
#         for c in range(n_cols):
#             ax = axes[r, c]
#             ax.axis("off")
#             ds = arr_ds[r, c]
#             if ds is not None:
#                 da = ds["precipRateNearSurface"]
#                 da.plot.imshow(ax=ax, cmap=cmap, norm=norm,
#                                add_colorbar=False, add_labels=False)

#     plt.tight_layout()
#     plt.show()



# def plot_node_samples_and_maps(arr_df, df, zarr_directory, figs_som_dir, variable="precipRateNearSurface", Ncols=5, num_images=25):
#     n_rows, n_cols = arr_df.shape
#     for r in range(n_rows):
#         for c in range(n_cols):
#             df_node = arr_df[r, c]
#             if df_node.empty:
#                 continue

#             img_fpath = os.path.join(figs_som_dir, f"node_{r}_{c}_samples.png")
#             img_fpath_map = os.path.join(figs_som_dir, f"node_{r}_{c}_map.png")

#             try:
#                 random_indices = random.sample(range(len(df_node)), min(num_images, len(df_node)))
#                 list_ds = []

#                 for index in random_indices:
#                     patch_row = df_node.iloc[index]
#                     granule_id = str(patch_row["gpm_granule_id"])
#                     patch_id = patch_row["patch_id"]
#                     time = pd.to_datetime(patch_row["time"])
#                     year, month = time.year, time.month

#                     search_path = os.path.join(zarr_directory, f"{year:04d}/{month:02d}", "*.zarr")
#                     zarr_files = glob.glob(search_path)

#                     for zarr_file in zarr_files:
#                         if granule_id in os.path.basename(zarr_file):
#                             ds = xr.open_zarr(zarr_file)
#                             list_ds.append(ds.isel(patch=patch_id))
#                             break

#                 fig = plot_images(list_ds, ncols=Ncols, figsize=(15, 15), variable=variable)
#                 fig.tight_layout()
#                 fig.savefig(img_fpath)
#                 plt.close(fig)

#                 df_subset = df_node.copy()
#                 df_subset["time"] = pd.to_datetime(df_subset["time"])
#                 df_subset["month"] = df_subset["time"].dt.month
#                 lon, lat = df_subset["lon"].values, df_subset["lat"].values

#                 fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={"projection": ccrs.PlateCarree()})
#                 plot_cartopy_background(ax)
#                 sc = ax.scatter(lon, lat, transform=ccrs.PlateCarree(), c=df_subset["month"], s=2)
#                 cbar = plt.colorbar(sc, ax=ax, orientation="vertical", pad=0.02)
#                 cbar.set_label("Month")
#                 fig.savefig(img_fpath_map)
#                 plt.close(fig)

#             except Exception as e:
#                 print(f"⚠️ Error in node ({r},{c}): {e}")


# def plot_mean_heatmap(arr_df, variable="P_mean", cmap="viridis"):
#     n_rows, n_cols = arr_df.shape
#     mean_values = np.full((n_rows, n_cols), np.nan)

#     for r in range(n_rows):
#         for c in range(n_cols):
#             df_node = arr_df[r, c]
#             if not df_node.empty:
#                 mean_values[r, c] = df_node[variable].mean()

#     masked = np.ma.masked_invalid(mean_values)
#     plt.figure(figsize=(8, 8))
#     plt.imshow(masked, cmap=cmap, origin="upper")
#     cbar = plt.colorbar()
#     cbar.set_label(f"Mean {variable}")
#     plt.title(f"Mean {variable} per SOM Node")
#     plt.xlabel("SOM Column")
#     plt.ylabel("SOM Row")
#     plt.xticks(np.arange(n_cols))
#     plt.yticks(np.arange(n_rows))
#     plt.grid(False)
#     plt.show()

    
# def compute_bmus(data, weights):
#     # data: (n_samples, n_features)
#     # weights: (n_rows, n_cols, n_features)
#     n_rows, n_cols = weights.shape[:2]
#     flat_weights = weights.reshape(n_rows * n_cols, -1)  # (n_nodes, n_features)

#     # Compute Euclidean distance to each SOM node
#     dists = np.linalg.norm(data[:, None, :] - flat_weights[None, :, :], axis=2)  # (n_samples, n_nodes)

#     bmu_indices = np.argmin(dists, axis=1)  # (n_samples,)
#     bmu_rows, bmu_cols = np.divmod(bmu_indices, n_cols)
#     return np.stack([bmu_rows, bmu_cols], axis=1)  # (n_samples, 2)

# def update_dataframe_with_bmus(df, features, weights):
#     bmus = compute_bmus(features, weights)
#     df["row"] = bmus[:, 0]
#     df["col"] = bmus[:, 1]
#     return df


# df_sample = update_dataframe_with_bmus(df_sample, X, W_trained)

# arr_df, arr_ds = create_node_image_array(W_trained, df_sample)
# plot_som_grid(arr_ds, cmap="turbo", norm=colors.LogNorm(vmin=0.01, vmax=300))
# plot_node_samples_and_maps(arr_df, df_sample, zarr_directory, figs_som_dir)
# plot_mean_heatmap(arr_df, variable="P_mean")
